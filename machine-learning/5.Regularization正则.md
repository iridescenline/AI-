## regression example:
exp1:
technical term: underfit or high bias 
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091257954.png)

exp2: just right
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091257334.png)

this model seems to fit the training set not perfectly but pretty well , will generalize well to new examples.
the learning algorithm to do well even on examples that are not on the training set -----called generalization .

exp3:  overfit  high variance
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091257088.png)
fit the data almost too well hence it's overfit and it not look like this model will generalize to new examples it has never seen before.

![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091257000.png)


## Addressing overfitting
option1: collect more training examples
option2: select features to include/exclude-->feature selection
all features + insufficient data -> overfitting
disadvantage:useful features could be lost
option3: regularization-->reduce size of parameters
keep all features but it just prevent the features from having a overly large effect which is what sometimes can cause overfitting . 
--->
## Regularization
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091257876.png)



### Regularized linear regression

![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091257504.png)

![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091257767.png)


### Regularized logistic regression
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091258351.png)
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091258357.png)