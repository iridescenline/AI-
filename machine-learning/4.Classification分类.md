if we still use linear regression ,we would find some problem.
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091255219.png)

# Logistic regression

sigmoid function
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091255107.png)
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091255747.png)


### Decision boundary决策边界
to get a better sense of how logistic regression is computing its predictions.


![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091255662.png)

exp1:
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091255588.png)

exp2:
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091256350.png)

more complex boundary exp:
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091256215.png)

### 代价函数
for拟合逻辑回归模型的参数
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091256562.png)
as for linear regression model , the cost function we have defined is squared error function.
当我们将${h_\theta}\left( x \right)=\frac{1}{1+{e^{-\theta^{T}x}}}$带入到这样定义了的代价函数是，得到的是一个非凸函数
non-convex   function.

![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091256287.png)
这意味着我们的代价函数有许多局部最小值，这将影响下降算法找到全局最小值。
线性回归的代价函数为：$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{\frac{1}{2}{{\left( {h_\theta}\left({x}^{\left( i \right)} \right)-{y}^{\left( i \right)} \right)}^{2}}}$ 。
我们重新定义逻辑回归的代价函数为：$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{{Cost}\left( {h_\theta}\left( {x}^{\left( i \right)} \right),{y}^{\left( i \right)} \right)}$，其中

![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091256855.png)


$h_θ{(x)}$与 $Cost(h_θ{(x)},y)$之间的关系如下图所示：
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091257804.png)

这样构建的$Cost\left( {h_\theta}\left( x \right),y \right)$函数的特点是：
当实际的 y=1 且${h_\theta}\left( x \right)$也为 1 时误差为 0，当 y=1 但${h_\theta}\left( x \right)$不为1时误差随着${h_\theta}\left( x \right)$变小而变大；
当实际的 y=0 且${h_\theta}\left( x \right)$也为 0 时代价为 0，当$y=0$ 但${h_\theta}\left( x \right)$不为 0时误差随着 hθ(x)的变大而变大。 将构建的  $Cost(h_θ{(x)},y)$ (loss function)简化如下：
$Cost(h_θ{(x)},y)$=${-{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)$
带入代价函数cost function得到即得：$J\left( \theta \right)=-\frac{1}{m}\sum\limits_{i=1}^{m}{[{{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)+\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)]}$


### 梯度下降 gradient descent

gradient descent for logistic regression
Ps: looks like linear regression
$$w_j=w_j-\alpha[{\frac{1}{m}\sum_{i=1}^m(f_{\vec{w},b}({\vec{x}^{(i)}})-y^{(i)})x_j^{(i)}}]$$


$$b=b-\alpha[\frac{1}{m}\sum_{i=1}^m(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})]$$

comparision:
linear regression :  $$f_{\vec{w},b}(\vec{x})=\vec{w}\cdot\vec{x}+b$$
logistic regression:
$$f_{\vec{w},b}(\vec{x})=\frac{1}{1+e^{-(\vec{w}\cdot{\vec{x}+b)}}}$$


### Cost function with regularization
$$J(w,b)=\frac{1}{2m}\sum\limits_{i=1}^{n}(f_{w,b}(x^{(i)})-y^{i})^2+\frac{\lambda}{2m}\sum_{j=1}^n{w_j}^2$$


