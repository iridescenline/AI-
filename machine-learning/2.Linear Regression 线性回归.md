### Linear regression model
$f_{(w,b)}{(x)}=wx+b$

### Cost function 代价函数
 $$J(w,b)=\frac{1}{2m}\sum\limits_{i=1}^{n}(f_{w,b}(x^{(i)})-y^{i})^2$$
$$f_{w,b}(x^{(i)})即是\widehat{y^{(i)}}$$

代价函数的直观表现

![Pasted image 20240922160942](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091250796.png)

![Pasted image 20240922161025](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091250118.png)





==minimize  $J(\theta_0,\theta_1)\rightarrow$== 
### Gradient descent 梯度下降
repeat util convergence

![Pasted image 20240922161711](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091251514.png)


![Pasted image 20240922162253](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091251099.png)

![[Pasted image 20240922162317.png]]

![Pasted image 20240922162337](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091251356.png)
![Pasted image 20240922162542](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091251175.png)

![Pasted image 20240922162634](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091252956.png)


![Pasted image 20240922163851](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091251086.png)
![Pasted image 20240922164542](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091252563.png)
sum :  batch gradient descent , each step of gradient descent uses all training data examples.


![Pasted image 20240922164420](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091252510.png)


