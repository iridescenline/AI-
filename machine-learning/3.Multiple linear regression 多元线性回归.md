previous:
$$f_{w,b}(x)=wx+b$$
now:
$$f_{w,b}(x)=w_1x_1+w_2x_2+……w_nx_n+b$$
### vectorized:
$$f_{\vec{w},b}(\vec{x})=\vec{w}\vec{x}+b$$![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091252120.png)


code practice:
using numpy.dot() is greatly faster than for loop.

### gradient descent
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091252870.png)

![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091252371.png)

### Normal equation : an alternative to gradient descent
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091253494.png)


# Feature scaling特征缩放
take features that take on very different ranges of values, and scale them to have comparable
ranges of values to each other. 
aim:be able to get gradient descent to run much faster 
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410161951150.png)

how to do
method1:original value divide by maximum of the range.
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091253178.png)

method2:Mean normalization均值归一化
find the average : $\mu$
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091253441.png)
method3: Z-score normalization
standard deviation : $\sigma$

![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091253588.png)


sum:
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091253660.png)


### Learning curve


![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091253480.png)

another way to decide when you model is done training is with an 
**automatic convergence test**
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091253681.png)

#### how to choose a good learning rate
in normal situation , with a small enough $\alpha$ ,$J(\vec{w},b)$ should decrease on every iteration .
otherwise,there may have some bug in code.
list a few wrong example:
exp1:
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091253588.png)

exp2:
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091254031.png)
every iteration,the vertical value jump here and there : $\alpha$ is too big

exp3:
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091254588.png)
1.bug in code
2.$\alpha$ is too large
3.use a plus sign in formation
$w_1=w_1+{\alpha} d_1$

sum:
set $\alpha$ to be  small 
if too small , then gradient descent would take a lot of iterations to converge.
so,try a range of values for the learning rate.
values of $\alpha$ to try:
$$ 0.001\quad 0.003\quad 0.01\quad 0.03\quad 0.1\quad 0.3\quad 1\quad 3\quad ……$$

### Feature engineering
Using intuition to design new features , by transforming or combing original features.

### Polynomial Regression
integrate multiple linear regression and  feature engeering 
to come up this new algorithm -->polynomial regression
let you fit curves non-linear functions to data set

