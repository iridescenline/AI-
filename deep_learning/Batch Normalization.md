在深度学习当中，存在一种重要的优化/思想，即批量归一化。
批量归一化使超参数搜索问题（hyperparameter search problem )变得更简单，使神经网络更稳定地更有效地选择超参数（超参数地范围会更大，并且工作效果良好）。让我们更加容易地训练神经网络（包括深度神经网络）。

### 优点一：高效性
举个例子，在逻辑回归的模型当中，我们通过归一化处理输入特征x，帮助我们更有效地训练w和b。
而在批量归一化当中，在一个四层网络当中（如下图），这时我们要训练的是$w^{[l]}$和$b^{[l]}$的值。很自然的想到，我们是否能通过归一化处理每一层隐藏层的$a^{[l]}$，从而使我们更有效地训练$w^{[l]}$和$b^{[l]}$？

Ps:在深度学习界中，对归一化处理$z^{[l]}$还是$a^{[l]}$存在争议，在这里，吴恩达老师讲的是归一化处理$z^{[l]}$的版本。

![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410071636528.png)
上图展示的是对$z^{[l]}$进行归一化处理的过程。
下图为计算流程：

![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410071714553.png)

在批量归一化处理时，$b^{[l]}$会被约去，所以我们一般直接把$b^{[l]}$设为0。
同时，Batch Normalization 和mini-batche 经常一起使用。

![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410071737818.png)

### 优点二：
限制前层在的参数更新对本层数值分布的影响。

### 优点三：
具有轻微的正则优化的效果。（类比dropout）（Ps:dropout知识补充ing）
但不要把批量归一化当作正则来用。
只把它看成归一化隐藏单元的激活值，并加速学习的方式。