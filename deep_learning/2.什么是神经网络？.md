
the term deep learning refers to training neural networks.
深度学习指的是训练神经网络。

例子：房屋价格预测
一个最简单的神经网络，输入房屋大小，通过单个“神经元”，完成线性运算，最终输出一个价格。
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410061140038.png)

而复杂一点的神经网络便是通过多个独立的”神经元“堆叠起来的。
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410061151991.png)
而在神经网络当中，我们要做的就是，输入x，就能得到结果y。不管训练集有多大，所有的中间过程，它都会自己完成。
在神经网络当中，只要喂给它足够多的关于输入x和输出y的数据，再通过调整权重和偏差，它会自动算出从x到y的精准映射函数。

term: 中间的三个圆圈被称为”隐藏单元“
Ps:神经网络的第一层用于接收输入数据 x ，最后一层用于输出 y,而中间层并没有明确的输入和输出（这部分的输入输出由机器自己决定），因此把这些中间层称为“隐藏层”，隐藏层的结点称作”隐藏单元“。
隐藏层的作用，解决线性模型的局限性，使神经网络得以处理更加复杂的函数关系。

上图中的直线便是在神经网络当中常见的ReLu函数。
先简单介绍一下，
ReLU，全称为：Rectified Linear Unit（修正线性单元），是一种人工神经网络中常用的激活函数，通常意义下，其指代数学中的斜坡函数，即
$$f(x)=max(0,x)$$
对应的函数图像如下所示：

![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410061145377.png)



# 速看版：
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410061229465.png)

第一层接收数据，并将数据按照不同权重传递给隐藏层的相应神经元，隐藏层的输出由激活函数所决定，隐藏层还会将输出按照相应权重传递给输出层，输出层根据激活函数产生最终的结果。

那么，神经网络模型是如何通过训练提高准确度的？
提高准确度，是通过调整每一层的权重，偏差或者激活函数来进行的。
## Back Propagation 逆向参数调整法

作用：优化权重和偏差

初始阶段，可以先将偏差设为0，并随机设置权重值。接着通过不断调整权重和偏差，使得输出和期待结果的误差平方和最小。也即$S=\sum_{i=1}^n(E_i-y_i)^2$最小。调整的方法即被称为逆向参数调整法。


具体调解过程中，我们需要用到两个方法，一个是梯度下降法，一个是链式法则。
1.使用链式法则计算导数
2.将导数插入梯度下降中来优化参数（即权重，偏差）
## 激活函数
#### 隐藏层激活函数的选择
在没有激活函数的情况下，无论如何调整权重，其输出值仍为线性，加入更多的隐藏层本质上也是一样的。而真实世界中的大多数系统是非线性的。如果要模拟复杂的系统，就必须借助非线性的激活函数。
1.Sigmoid 函数
$$\frac{1}{1+e^{-x}}$$
![](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410091300444.png)



具有以下特点：（1）其输出值落于0~1连续区间 
             （2）在输入值从横坐标左侧移动到右侧的过程中，其输出值呈现从平缓到加速再到平缓的特点
             （3）该函数的导数值位于0~0.25的连续区间。

综上，Sigmoid函数在神经网络当中，其实具有很大的局限性，在逆向参数调整 Back Propagation过程中，使用链式法则，可以推到出下面的公式，并对深度神经网络模型的权重调整幅度进行计算，

![32936d2e2031549289b204826854799.jpg](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/32936d2e2031549289b204826854799.jpg)

红框内容是各层Sigmoid激活函数导数的乘积，多个小于0.25的值相乘之后，会严重影响最终的权重调整幅度，这也就导致在神经网络模型的训练当中，第一层的初始权重之后很难通过逆向调整再调节到一个合适的权重值，这个问题被称为==梯度消失==。
另外，Sigmoid函数在进行指数运算时，需要消耗较多的算力资源，因此在平常的神经网络ANN模型中，它通常不被使用。

2.Tanh函数
$$\frac{e^x-e^{-x}}{e^x+e^{-x}}$$
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410061640911.png)

具有以下特征：（1）其输出值落于-1~1的区间
			 （2）其导数值落于0~1区间内，大于Sigmoid的导数区间，因此相比Sigmoid ,Tanh相对更够缓解梯度消失的问题。

3.ReLu函数
在实际应用中，隐藏层的默认推荐激活函数通常为ReLu。
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410061644018.png)
（1）首先，它是一个非线性函数，并且其在大于0时展现的线性特征能够很好地解决梯度消失的问题。
（2）另外，相较于Sigmoid和Tahn，ReLu带来了更高效的计算。
（3）最后，根据通用近似定理，其整体的非线性特征，又能在神经网络中拟合任何复杂的连续函数。
![0423f24a4ba7d6f292875fffcc27d1b.jpg](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410061648078.jpg)
例如，在上图中，通过不断堆叠ReLu函数，可以拟合出一个非线性的三次函数。
（4）不过，在x负半轴上，ReLu函数的输出值和导数均为零，这意味着该神经元处于熄灭状态，且在逆向参数调整过程中不产生梯度调整值。


4.Leaky ReLu(ReLu的变体)
在负数区间内，通过添加了一个较小斜率的线性部分，使负数区域内也能产生梯度调整值，完美解决之前的神经元熄灭问题。
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410061717658.png)

以上4种函数在隐藏层的选择顺序如下
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410061718730.png)

#### 输出层激活函数的选择
隐藏层的数据最终来到输出层，通过输出层的激活函数转化成业务要求的表达形式，因此输出层激活函数的选择时以业务要求为导向的。
（1）二分类问题，target：判断图片中的动物是否是猫。则选择 Sigmoid函数 返回是猫的概率作为最终输出值。
（2）多分类问题，target：输出图片分别为猫狗鼠的概率。则选择SoftMax函数，返回属于每个类别的概率，其概率总和为1。
（3）多标签问题（样本属于多个类别的情况，例如，一张图片种既有猫，又有狗），target：计算图片中分别出现猫狗鼠的概率。则选择Sigmoid 函数对每个类别的概率单独进行计算。
（4）线性回归问题，target：预测绝对的数值。则可以选择线性函数作为激活函数。
## 补充知识
### 通用近似定理 Universal Approximation Theorem
神经网络中至少需要一层隐藏层，和足够多的神经元，利用非线性的激活函数，便可以模拟任何复杂的连续函数。

### SoftMax函数(1s了解版)

对于二分类问题，我们可以使用sigmoid函数（也即是逻辑函数），将$(-\infty,+\infty)$范围内的数值映射成一个0-1区间内的值，并且用来表示概率。

但如果我们遇到多分类问题，其中一种常用的方法便是Softmax函数，它可以预测每个类别的概率，并且这些类概率之和为1。s
### 神经网络层数的数法
如下图所示，
输入层称为第0层，不计入总层数；
隐藏层为第1层，而输出层为第二层。
所以，下图是一个二层神经网络。
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410062016516.png)


### 核对矩阵的维数

向量化之后，
$$Z^{[i]}=w^{[i]}X+b^{[i]}$$

$$w^{[i]}的维度为（n^{[i]},n^{[i-1]}）$$

$$X的维度是（n^{[i-1]},m）$$

$$b^{[i]}的维度为（n^{[i]},1）$$

$$Z^{[i]}的维度为（n^{[i]},m）$$

因为 $z^{[l]}=g^{[i]}(a^{[i]})$，所以$a^{[i]}$维度和$z^{[i]}$是一样的。
