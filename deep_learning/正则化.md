遇到高方差问题时，我们可以尝试正则化来减少方差，避免过度拟合。

目的：$minJ(w,b)$

## 采取$L_2$正则化

在逻辑回归中，
逻辑回归成本函数变成
$$J(w,b)=\frac{1}{m}\sum_{i=1}^{m}L(\hat{y^{(i)}}-y^{(i)})+\frac{\lambda}{2m}\parallel{w}\parallel^2+\frac{\lambda}{2m}b^2$$
**$\frac{\lambda}{2m}b^2$一般会忽略**

迁移到神经网络中

$$J(w^{[1]},b^{[1]},...,w^{[l]},b^{[l]})=\frac{1}{m}\sum_{i=1}^{m}L(\hat{y^{(i)}}-y^{(i)})+\frac{\lambda}{2m}\parallel{w}\parallel_F^2$$
其中，$$\parallel{w}\parallel_F^2 = \sum_{i=1}^{n^{[l-1]}}\sum_{j=1}^{n^{[l]}}(w_{ij}^{[l]})^2$$
该矩阵范式被称为“佛罗贝尼乌”范数，而不称作$L_2$范数，

在正规化后，成本函数新增了一个$\frac{\lambda}{2m}\parallel{w}\parallel_F^2$,
那么
$$dw^{[l]}= (from backpop)+ \frac{\lambda}{m}w^{[l]}$$
$$w^{[l]}=w^{[l]}-\alpha{dw^{[l]}}$$

把上面两个式子整合一下得到
$$w^{[l]}=(1-\frac{\alpha\lambda}{m})w^{[l]}-\alpha(from backpop)$$
没有使用正则化之前，应该为
$$w^{[l]}=w^{[l]}-\alpha(from backpop)$$
相当于$w^{[l]}$前新增了一个小于1的系数，所以$L_2$正则化已被称作权重衰减。


## 采用dropout正则化

作用：防止神经网络过拟合。

原理：在训练过程中，随机地将一定比例的神经元“丢弃”（即设置为零），这样可以减少神经元之间的相互依赖。通过这种方式，模型在每次迭代中只使用部分神经元，促进了特征的独立性和鲁棒性。最终，在测试阶段，所有神经元都参与计算，从而增强模型的泛化能力。


**缩放输出**：在测试阶段，所有神经元都参与计算，但为了保持输出的一致性，通常会将训练阶段的输出乘以保留概率（如0.5），以抵消在训练时随机丢弃神经元的影响。（在训练时，Dropout通过随机丢弃部分神经元来防止过拟合，使模型变得更鲁棒。然而，在测试阶段，目标是评估模型在新数据上的性能，因此需要所有神经元一起工作，以便最大限度地提高输出的稳定性和准确性。）

一点点小bug：dropout影响了训练过程中成本函数的计算和优化。
一是Dropout通过随机丢弃神经元来引入不确定性。这使得每次前向传播时网络结构都不同，导致损失函数在训练过程中的计算变得更为复杂。
二是由于模型在训练时经常改变，因此损失函数可能会显示出更大的波动。这种波动可能导致优化算法在收敛过程中表现出不稳定性。


## 其他正则方法
### ”假“扩增数据 
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410171831837.png)
### 提早停止（early stopping）
在训练过程中监控模型在验证集上的性能，并在性能不再提高时停止训练。
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410171833482.png)

