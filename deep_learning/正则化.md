遇到高方差问题时，我们可以尝试正则化来减少方差，避免过度拟合。

目的：$minJ(w,b)$

采取$L_2$正则化

在逻辑回归中，
逻辑回归成本函数变成
$$J(w,b)=\frac{1}{m}\sum_{i=1}^{m}L(\hat{y^{(i)}}-y^{(i)})+\frac{\lambda}{2m}\parallel{w}\parallel^2+\frac{\lambda}{2m}b^2$$
**$\frac{\lambda}{2m}b^2$一般会忽略**

迁移到神经网络中

$$J(w^{[1]},b^{[1]},...,w^{[l]},b^{[l]})=\frac{1}{m}\sum_{i=1}^{m}L(\hat{y^{(i)}}-y^{(i)})+\frac{\lambda}{2m}\parallel{w}\parallel_F^2$$
其中，$$\parallel{w}\parallel_F^2 = \sum_{i=1}^{n^{[l-1]}}\sum_{j=1}^{n^{[l]}}(w_{ij}^{[l]})^2$$
该矩阵范式被称为“佛罗贝尼乌”范数，而不称作$L_2$范数，

在正规化后，成本函数新增了一个$\frac{\lambda}{2m}\parallel{w}\parallel_F^2$,
那么
$$dw^{[l]}= (from backpop)+ \frac{\lambda}{m}w^{[l]}$$
$$w^{[l]}=w^{[l]}-\alpha{dw^{[l]}}$$

把上面两个式子整合一下得到
$$w^{[l]}=(1-\frac{\alpha\lambda}{m})w^{[l]}-\alpha(from backpop)$$
没有使用正则化之前，应该为
$$w^{[l]}=w^{[l]}-\alpha(from backpop)$$
相当于$w^{[l]}$前新增了一个小于1的系数，所以$L_2$正则化已被称作权重衰减。

