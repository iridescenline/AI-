先是看了一遍b站新兴up主 statquest 的 从RNN--LSTM--self-attention到transformer的视频。
statquest其实前几年就在油管上发布过机器学习和深度学习的视频，只不过今年b站才有了搬运视频。
and 我觉得讲的比较通俗易懂，所以先看了一遍了解一下。

接着看了李宏毅的self-attention和transformer的视频（嗯，讲的也很清楚，台湾腔和闽南语让人熟悉。）
以上这部分用了三四天的零散时间。

然后开始看太阳花的视频，以及他分享的代码。
视频还好，代码是真的………（直到11.15，也就是昨天才跑起来）。
各种报错，因为一开始我连路径怎么上传都母鸡。
后来路径都配好了。
还是报错，
![0483c68b100ca7c412effe83cd96ba2.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202411162216692.png)
最后在hjj组长的提示下，试了把batch_size设为0，据说这样便只在主线程跑了。然后成功运行了。

以及第二天hjj组长发现，把worker的数量设为0，即使batch_size不为0，也能跑得起来。

总之，现在还是不大清楚为什么。
以及ai的回答是：
