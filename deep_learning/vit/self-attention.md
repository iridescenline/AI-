### 自注意力机制的特点
自注意力机制（self-attention）是深度学习中一种重要的机制，尤其在处理序列数据时非常有效。它能够捕捉输入序列中各个位置之间的关系。

### 自注意力机制的运用实例（多向量输入）：

1. **Transformer模型**：如BERT、GPT、T5等，这些模型都使用自注意力机制来对输入序列中的词进行编码，从而提升上下文理解能力。
2. **图像处理**：在Vision Transformer（ViT）中，图像被视为序列数据，利用自注意力机制来捕捉图像中的空间关系。
3. **语音识别**：在某些语音处理模型中，自注意力可以用来捕捉不同时间步之间的依赖关系。
4. **推荐系统**：用于处理用户历史数据，通过自注意力机制了解用户的偏好变化。

### 自注意力机制的输出
N to N
N to 1
N to N'  (seq2seq)

### 自注意机制的操作流程
自注意力机制的输出是通过计算输入序列中各个元素的加权和来获得的。

1. **输入表示**：首先，将输入序列中的每个元素转换为一个向量表示。通常来说，这些向量可以通过嵌入层（embedding layer）获得。
    
2. **创建查询、键、值**：

    - 对每个输入向量，生成三个不同的向量：查询（Query）、键（Key）和值（Value）。
    - 这通常是通过与一些可训练的权重矩阵进行线性变换来实现的。
3. **计算注意力权重**：$\alpha=q\cdot k$

    
    - 通过计算查询向量与所有键向量的相似度来获得注意力权重，常用的方法是点积（dot product）计算，接着使用softmax函数将相似度转换为概率分布。这些权重表示了各个元素对当前查询的相关性。
    
4. **加权求和**：
    
    - 将注意力权重与对应的值向量相乘，然后进行加权求和，得到输出向量。这个输出向量结合了输入序列中各元素的信息，以反映它们的上下文关系。


![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202411162028809.png)
图示最终输出结果为$b=softmax(\frac{q\cdot k}{\sqrt{d}})\cdot v$
Ps:为了避免得分过大，引入缩放因子$\sqrt{d}$

5. **多头注意力**（可选）：
    
    - 自注意力机制通常会使用多头注意力，意味着将输入分成多个不同的部分并分别计算自注意力，然后将所有输出拼接起来并线性转换，以增强模型的表达能力。

![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202411162034368.png)

$b^i=W^o\cdot(b^{i,1},b^{i,2})$


### 位置编码（position encoding）

自注意力机制本身并不一定需要位置编码（position encoding），但在处理序列数据时，位置编码通常是非常重要的。
位置编码可以提供关于输入序列中各个元素位置的信息。有了位置编码，模型能够更好地理解元素之间的相对位置关系，从而捕捉到序列中的时间或空间结构。

