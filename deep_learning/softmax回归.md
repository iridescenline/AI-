## 什么是 Softmax？
虽然名字是回归，但其实是一个分类。
## 概述


**Softmax** 是一种常用的激活函数，主要用于多类分类问题的输出层。它将模型的输出（logits）转换为概率分布，使得每个类别的预测值在 \(0\) 到 \(1\) 之间，并且所有类别的概率之和为 \(1\)。

## Softmax 的定义

对于一个输入向量 $( z = [z_1, z_2, \ldots, z_n] )$，Softmax 函数的计算公式为：

$sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}} \quad \text{对于每个 } i = 1, 2, \ldots,n$

### 主要特性

1. **输出范围**：
   - Softmax 的输出值在 \(0\) 到 \(1\) 之间，表示每个类别的概率。

2. **归一化**：
   - Softmax 会将所有输出归一化为概率分布，因此每个类的概率值与其他类的概率值相对。

3. **适用于多类分类**：
   - 在多类分类任务中，Softmax 常用在神经网络的最后一层，输出每个类别的概率，从而选择概率最大的类别作为预测结果。

## 示例

假设有一个神经网络的输出层计算得出以下得分：

$z = [2.0, 1.0, 0.1]$

使用 Softmax 函数进行计算：

1. 计算每个得分的指数：
$$e^{2.0} \approx7.39 $$
$$e^{1.0} \approx 2.72 $$
$$e^{0.1} \approx 1.11$$

2. 计算这些值的和：
   $$ 7.39 + 2.72 + 1.11 \approx 11.22 $$
3. 计算 Softmax 输出：
$$ \sigma(z_1) \approx \frac{7.39}{11.22} \approx 0.66$$
 $$\sigma(z_2) \approx \frac{2.72}{11.22} \approx 0.24  $$
  
$$\sigma(z_3) \approx \frac{1.11}{11.22} \approx 0.10  $$ 

最终输出的概率为：
- 类别1：0.66
- 类别2：0.24
- 类别3：0.10

## 总结

Softmax 函数将模型输出转换为概率分布，常用于多类分类问题的输出层。通过这种方式，网络可以提供每个类别的预测概率，便于选择最可能的类别。


当分类数为2，并使用了softmax，实际上就回到了逻辑回归的计算方式。


## 如何用损失函数来优化？
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410172226676.png)

这里损失函数称作交叉熵损失函数（Cross-Entropy Loss)
$$L(\hat{y},y)=-\sum_{j=1}^{4}y_jlog{\hat y_j}$$
交叉熵损失函数（Cross-Entropy Loss）是深度学习中常用的一种损失函数，特别是在分类问题中。它衡量的是模型预测的概率分布与真实标签的概率分布之间的差异。交叉熵损失函数通常用于多分类问题，特别是在使用 softmax 作为激活函数时。

- $y_j$是真实标签的第 ii 个元素。
- $\hat{y_j}$是模型预测的第 ii 个元素的概率。


