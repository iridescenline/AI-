{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf87cf5-226f-4f44-9193-1ec2daa19f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1325a9d-38fc-49a3-81e3-bdf6010b80a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape: \t torch.Size([1, 6, 28, 28])\n",
      "Sigmoid output shape: \t torch.Size([1, 6, 28, 28])\n",
      "AvgPool2d output shape: \t torch.Size([1, 6, 14, 14])\n",
      "Conv2d output shape: \t torch.Size([1, 16, 10, 10])\n",
      "Sigmoid output shape: \t torch.Size([1, 16, 10, 10])\n",
      "AvgPool2d output shape: \t torch.Size([1, 16, 5, 5])\n",
      "Flatten output shape: \t torch.Size([1, 400])\n",
      "Linear output shape: \t torch.Size([1, 120])\n",
      "Sigmoid output shape: \t torch.Size([1, 120])\n",
      "Linear output shape: \t torch.Size([1, 84])\n",
      "Sigmoid output shape: \t torch.Size([1, 84])\n",
      "Linear output shape: \t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape: \\t',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc0c641-b0ed-46d0-bc02-07fbc711b78b",
   "metadata": {},
   "source": [
    "## 模型训练，看看LeNet在Fashion-MNIST数据集上的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a5ee21b-1385-471b-ba23-510fee0d8029",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6e57c8-05f4-41c5-afc6-3ceaf48e8c54",
   "metadata": {},
   "source": [
    " 虽然卷积神经网络的参数较少，但与深度的多层感知机相比，它们的计算成本仍然很高，因为每个参数都参与更多的乘法。 通过使用GPU，可以用它加快训练。\n",
    " 由于完整的数据集位于内存中，因此在模型使用GPU计算数据集之前，我们需要将其复制到显存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232e8afd-7470-4389-904a-4783f811b2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gpu(net,data_iter,device=None):\n",
    "    \"\"\"使用GPU计算模型在数据集上的精度\"\"\"\n",
    "    if isinstance(net,nn.Moudule):\n",
    "        net.eval() # 设置成评估模式\n",
    "        if not device:\n",
    "            device=next(iter(net.parameters())).device\n",
    "    # 正确预测的数量，总预测的数量\n",
    "    metric=d2l.Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for X,y in data_iter:\n",
    "            if isinstance(X,list):    \n",
    "                #BERT微调所需的(之后将介绍）\n",
    "                X=[x.to(device) for x in X]\n",
    "            else:\n",
    "                X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            metric.add(d2l.accuracy(net(X),y),y.numel())\n",
    "        return metric[0]/metric[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
