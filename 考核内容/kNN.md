### k-nearest-neighbor
所谓k最近邻，说的是每个样本都可以用它最接近的k个邻近值来代表。
kNN作为监督学习的分类算法，需要有标签的训练数据，新样本的类别由距离新样本的k个最近邻训练样本按照分类决策规则（如多数决策）决定。
![Pasted image 20241005085318](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101039395.png)


KNN算法的思想非常简单：对于任意n维输入向量，分别对应于特征空间中的一个点，输出为该特征向量所对应的类别标签或预测值。


KNN算法是一种非常特别的机器学习算法，因为它没有一般意义上的学习过程。
k近邻法在训练阶段仅仅是把样本保存起来，训练时间开销为零，待收到测试样本后再进行处理。KNN能够快速高效地解决建立在特殊数据集上的预测分类问题，但其不产生模型，因此算法准确 性并不具备强可推广性。

### 距离的度量
在kNN算法中，有多种常见的距离度量方法，如欧几里得距离，曼哈顿距离等等。
#### 欧几里得距离
二维平面上两个点$a(x1,y1)，b(x2,y2)$间的欧几里得距离：
$$d=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$$
拓展到两个n维向量$a(x_{11},x_{12},\dots,x_{1n})$与$b(x_{21},x_{22},\dots,x_{2n})$间的欧几里得距离：
$$d=\sqrt{\sum_{k=1}^{n}(x_{1k}-x_{2k})^2}$$


#### 曼哈顿距离
$$d=|x_1-x_2|+|y_1-y_2|$$
这两种距离划分结果示意：
![09121b82f28bab6205cc34bd3012c76](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101039058.jpg)
Ps:左图为曼哈顿距离，右图为欧几里得距离。
通过观察，我们可以看到，两幅图决策边界的图形有差异，欧几里得距离的决策边界相较于曼哈顿距离较为平滑。
而在实际应用过程中，选择何种距离度量取决于问题和数据，最好是二者都尝试，看看哪种效果更好。


### k值的选取
k值太小，分类结果易受==噪声点==影响，波动较大；
k值太大，由于k个最近邻中可能包含了距离较远的，但不是同类的数据点，导致分类错误。
另外，我们一般也要选择K的值应该尽量选择为奇数，否则会出现同票的情况。

那么，我们如何能够选择一个合适的k值呢？
不存在一个绝对的方法可以确定k的最佳值，所以我们需要在确定k值前尝试几个值。
方法是假装部分训练数据是未知的，然后使用kNN对“未知数据”进行分类，然后评估分类结果与真实结果的匹配程度。


答案是交叉验证。
交叉验证指的是将训练数据集进一步分成训练数据和验证数据，选择在验证数据里面最好的超参数组合。
Ps:参数分为模型参数和超级参数，模型参数是需要我们通过不断的调整模型和超参数训练得到的最佳参数。而超参数则是我们人为手动设定的值。
在kNN中，k值便是一个超级参数（或者称为超参数）。我们可以选择一组最好的k值作为模型最终的k值。下图是五折交叉验证：
![Pasted image 20241005102210](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101039899.png)


### kNN算法的优缺点
优点：
1.简单易用。没有很复杂的数学原理。
2.模型训练时间快，训练时间开销为零。
缺点：
1.kNN算法属于懒惰算法，它在训练阶段所做的仅仅是将样本保存起来，如果训练集很大，必须使用大量的存储空间。
2.==维度诅咒==
需要训练样本相当密集地覆盖整个空间，意味着需要大量的呈现指数增长的训练样本，永远不会获得足够多的图片来实现密集覆盖。


### 优化：kd-tree（待填坑）
