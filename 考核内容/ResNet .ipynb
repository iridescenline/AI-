{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5928467-7fc4-48ce-9546-60390a1d439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab277f65-9e6f-4daf-9bb8-b23093e9ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, input_channels, num_channels, use_lxlconv=False, strides=1):\n",
    "        #初始化方法接收输入通道数,输出通道数,是否使用1x1卷积和步幅参数\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(inputinput_channels, num_channels, kernel_size=3, padding=1) # stride默认=1\n",
    "        if use_lxlconv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        #定义批归一化层\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)  \n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "        #设置ReLU激活函数\n",
    "        #同时设置当 inplace=True 时，激活函数（如ReLU）会直接在输入张量上进行操作，而不是创建一个新的张量。\n",
    "        #这意味着不再分配额外的内存来存储输出结果，而是直接修改输入数据。\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        #在前向传播中，输入X首先通过conv1和bn1，\n",
    "        #然后应用ReLU激活函数。接着，再通过conv2和bn2。\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X) #如果使用了conv3，则将输入X通过conv3进行处理\n",
    "        Y += X  #将Y与X相加，形成残差连接，最后应用ReLU激活函数返回结果。           \n",
    "        return F.relu(Y)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5650e94-cf17-4a69-aecf-5ba839d6c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##same convolution\n",
    "#blk = Residual(3,3)  \n",
    "##创建残差块\n",
    "#X = torch.rand(4,3,6,6)  \n",
    "#Y=blk(X) #调用 blk 对象的 forward 方法，将输入张量 X 传递给这个残差块进行处理。\n",
    "#Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312fcaa6-3e1d-4e14-a995-26dd26131c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "##先是经过cov1,输出形状是(4,6,3,3)\n",
    "##经过cov2,输出形状还是(4,6,3,3) \n",
    "##然后再经过cov3\n",
    "#blk2 = Residual(3, 6, use_lxlconv=True, strides=2)\n",
    "#blk(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c0c91-d65a-437d-a7d9-9b445deadeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#blk3 = Residual(3, 6, use_lxlconv=True, strides=1)\n",
    "#Y = blk3(X)\n",
    "#Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0559f0-2432-4940-aaaa-8844870f342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet模型\n",
    "\n",
    "#定义一个网络的初始层b1\n",
    "b1 = nn.Sequential(nn.Conv2d(1,64,kernel_size=7,stride=2,padding=3), # 输入通道数是1,输出通道数是64\n",
    "                   nn.BatchNorm2d(64),nn.ReLU(), #批量归一化,引入非线性\n",
    "                   nn.MaxPool2d(kernel_size=3,stride=2,padding=1)) #最大池化\n",
    "\n",
    "# 构建多个残差块\n",
    "def resnet_block(input_channels, num_channels, num_residuals,  first_block=False):\n",
    "    blk = []  \n",
    "    for i in range(num_residuals):\n",
    "        if i==0 and not first_block: \n",
    "            #高宽减半\n",
    "            blk.append(Residual(input_channels, num_channels, use_lxlconv=True,strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels))\n",
    "    return blk\n",
    "\n",
    "#b2两个残差块都不减半( 原因是 b1 高宽减半2次） \n",
    "#剩下的b3到b5,第一个残差块减半,第二个不减半\n",
    "b2 = nn.Sequential(*resnet_block(64,64,2,first_block=True)) \n",
    "b3 = nn.Sequential(*resnet_block(64,128,2))\n",
    "b4 = nn.Sequential(*resnet_block(128,256,2))\n",
    "b5 = nn.Sequential(*resnet_block(256,512,2))\n",
    "\n",
    "net = nn.Sequential(b1,b2,b3,b4,b5,nn.AdaptiveAvgPool2d((1,1)), nn.Flatten(), nn.Linear(512,10))\n",
    "#将前面定义的模块组合在一起，构建了一个完整的深度残差网络（ResNet）\n",
    "#nn.AdaptivaAvgPool2d(((1,1)) 自适应平均池化层\n",
    "#该层将特征图的空间尺寸自适应地调整为1x1,不论输入特征图的尺寸是多少，输出都将是一个1x1的特征图通道数是512\n",
    "#nn.Flatten() 展平层\n",
    "#将1x1的特征图展平为一维向量。对于512个通道数，展平后将得到一个长度为512的一维向量。\n",
    "#nn.Linear(512,10) 全连接层\n",
    "#这个全连接层将展平后的特征向量转换为输出的类别数。这里输出大小为10，用来处理10个不同类别的图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc2f1e4-7b58-4137-a2fb-9d9582a572fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(size=(1,1,256,256))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf2f2c-8fdf-4f3d-8b3c-eae5a9765594",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.05, 10, 256\n",
    "#lr是学习率,设置成0.05\n",
    "#num_epochs 训练的轮次 设置为10\n",
    "#batch_size 每个批次样本的数量 \n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,resize=96)\n",
    "#d2l.load_data_fashion_mnist是一个用于加载 Fashion MNIST 数据集的函数\n",
    "#batch_size,指定每个批次的大小\n",
    "#resize=96,将图像大小调整为96x96像素,以便适应网络输入的要求。\n",
    "#返回值train_iter:用于迭代训练数据的迭代器\n",
    "#返回值tes_iter：用于迭代测试数据的迭代器\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs,  lr, d2l.try_gpu())\n",
    "#d2l.train_ch6 是一个用于训练模型的函数。\n",
    "#net是要训练的残差模型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
