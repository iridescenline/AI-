# Fully Convolutional Networks
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410111754924.png)

CNN层：和之前ResNet 卷积层的训练相似。（把图片缩小）
1x1 Cov : 用来减少通道数。
Transposed conv ：把图片还原成输入时的维度，k是通道数，表示是k类分类。

参考资料：[全卷积网络 FCN 详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/30195134)
## CNN与FCN
与经典的CNN在卷积层之后使用全连接层得到固定长度的特征向量进行分类（全连接层＋softmax输出）不同，FCN可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。
最后逐个像素计算softmax分类的损失, 相当于每一个像素对应一个训练样本。
FCN与CNN的区别在把于CNN最后的全连接层换成卷积层，输出的是一张已经Label好的图片。
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410161650514.png)

## CNN 的优劣点

CNN的强大之处在于它的多层结构能够自动学习特征，并且可以学到多个层次的特征：较浅的卷积层感知域小，能学到一些局部的特征，而较深的卷积层感知域大，能学到一些更加抽象的特征。这些抽象特征对物体的大小，位置和方向等敏感度更低，有助于图像内容的分类。与此同时，抽象特征丢失了一些物体的细节，不能很好地给出物体地具体轮廓，指出每个像素具体属于哪个物体，因此难以做到精确的分割。

**传统的基于CNN的分割方法**：为了对一个像素分类，使用该像素周围的一个图像块作为CNN的输入用于训练和预测。这种方法有几个缺点：一是存储开销很大。例如对每个像素使用的图像块的大小为15x15，然后不断滑动窗口，每次滑动的窗口给CNN进行判别分类，因此则所需的存储空间根据滑动窗口的次数和大小急剧上升。二是计算效率低下。相邻的像素块基本上是重复的，针对每个像素块逐个计算卷积，这种计算也有很大程度上的重复。三是像素块大小的限制了感知区域的大小。通常像素块的大小比整幅图像的大小小很多，只能提取一些局部的特征，从而导致分类的性能受到限制。

## FCN


