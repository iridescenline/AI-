# Fully Convolutional Networks
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410111754924.png)

CNN层：和之前ResNet 卷积层的训练相似。（把图片缩小）
1x1 Cov : 用来减少通道数。
Transposed conv ：把图片还原成输入时的维度，k是通道数，表示是k类分类。

参考资料：[全卷积网络 FCN 详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/30195134)
		  https://www.youtube.com/watch?v=Ahge3GzQ3Kg
## CNN与FCN
与经典的CNN在卷积层之后使用全连接层得到固定长度的特征向量进行分类（全连接层＋softmax输出）不同，FCN可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。
最后逐个像素计算softmax分类的损失, 相当于每一个像素对应一个训练样本。
FCN与CNN的区别在把于CNN最后的全连接层换成卷积层，输出的是一张已经Label好的图片。
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410161650514.png)

## CNN 的优劣点

CNN的强大之处在于它的多层结构能够自动学习特征，并且可以学到多个层次的特征：较浅的卷积层感知域小，能学到一些局部的特征，而较深的卷积层感知域大，能学到一些更加抽象的特征。这些抽象特征对物体的大小，位置和方向等敏感度更低，有助于图像内容的分类。与此同时，抽象特征丢失了一些物体的细节，不能很好地给出物体地具体轮廓，指出每个像素具体属于哪个物体，因此难以做到精确的分割。

**传统的基于CNN的分割方法**：为了对一个像素分类，使用该像素周围的一个图像块作为CNN的输入用于训练和预测。这种方法有几个缺点：一是存储开销很大。例如对每个像素使用的图像块的大小为15x15，然后不断滑动窗口，每次滑动的窗口给CNN进行判别分类，因此则所需的存储空间根据滑动窗口的次数和大小急剧上升。二是计算效率低下。相邻的像素块基本上是重复的，针对每个像素块逐个计算卷积，这种计算也有很大程度上的重复。三是像素块大小的限制了感知区域的大小。通常像素块的大小比整幅图像的大小小很多，只能提取一些局部的特征，从而导致分类的性能受到限制。

## FCN

### FCN的原理
FCN将传统CNN中的全连接层转化成卷积层，对应CNN网络FCN把最后三层全连接层转换成为三层卷积层。在传统的CNN结构中，前5层是卷积层，第6层和第7层分别是一个长度为4096的一维向量，第8层是长度为1000的一维向量，分别对应1000个不同类别的概率。
FCN将这3层表示为卷积层，卷积核的大小 (通道数，宽，高) 分别为 (4096,1,1)、(4096,1,1)、(1000,1,1)。
看上去数字上并没有什么差别，但是卷积跟全连接是不一样的概念和计算过程，使用的是之前CNN已经训练好的权值和偏置，但是不一样的在于权值和偏置是有自己的范围，属于自己的一个卷积核。因此FCN网络中所有的层都是卷积层，故称为全卷积网络。

![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410161725662.png)


### FCN的过程
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410161726497.png)
在上图中，经过多次卷积和pooling以后，得到的图像越来越小，分辨率越来越低。其中图像到 H/32∗W/32 的时候图片是最小的一层时，所产生图叫做heatmap热图，热图就是我们最重要的高维特诊图，得到高维特征的heatmap之后就是最重要的一步也是最后的一步对原图像进行upsampling，把图像进行放大、放大、放大，到原图像的大小。

最后通过逐个像素地求其在1000张图像该像素位置的最大数值描述（概率）作为该像素的分类。因此产生了一张已经分类好的图片，如下图右侧有狗狗和猫猫的图。

![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410161728160.png)
Ps:在真正代码当中第一层是1/2，以此类推。


对原图像进行卷积conv1、pool1后原图像缩小为1/2；之后对图像进行第二次conv2、pool2后图像缩小为1/4；接着继续对图像进行第三次卷积操作conv3、pool3缩小为原图像的1/8，此时保留pool3的featureMap；接着继续对图像进行第四次卷积操作conv4、pool4，缩小为原图像的1/16，保留pool4的featureMap；最后对图像进行第五次卷积操作conv5、pool5，缩小为原图像的1/32，然后把原来CNN操作中的全连接变成卷积操作conv6、conv7，图像的featureMap数量改变但是图像大小依然为原图的1/32，此时图像不再叫featureMap而是叫heatMap。

现在我们有1/32尺寸的heatMap，1/16尺寸的featureMap和1/8尺寸的featureMap，1/32尺寸的heatMap进行upsampling操作之后，因为这样的操作还原的图片仅仅是conv5中的卷积核中的特征，限于精度问题不能够很好地还原图像当中的特征，因此在这里向前迭代。把conv4中的卷积核对上一次upsampling之后的图进行反卷积补充细节（相当于一个差值过程），最后把conv3中的卷积核对刚才upsampling之后的图像进行再次反卷积补充细节，最后就完成了整个图像的还原。


### FCN的作用总结
FCN（全卷积网络）相比于普通CNN在图像语义分割任务中有几个显著的优势：

1. **像素级分类**（pixel-wise semantic segmentation）：FCN使用了转置卷积，将经过卷积层的图像解码回原始的分辨率。
 可以直接输出与输入图像相同尺寸的特征图，实现像素级的分类（识别每一个像素所在的类），而普通CNN通常只能提供图像级的分类结果。
    
2. **空间信息保持**：由于FCN的全卷积架构，网络能够保持输入图像的空间信息，避免了全连接层引入的位置信息丢失。
    
3. **灵活性**：之前学的VGG和ResNet都只能处理固定大小的输入图像（因为它们最后都使用到了全连接层fully connected layer)，而FCN (fully convolutional network) 可以处理任意尺寸的输入图像，这使得它在不同应用场景中具有更大的灵活性。

4. **上下文信息**：FCN通过使用多个卷积层和跳跃连接，能够有效捕捉上下文信息，从而改善分割的精细度和准确性。
    
5. **计算效率**：FCN避免了全连接层的高计算成本，尤其是在处理大尺寸输入时，使用卷积层的计算效率更高。