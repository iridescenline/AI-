### 导入
理论上，随着网络深度的加深，应该训练得越来越好才对。但是根据实际经验，我们会发现随着网络深度的加深，训练错误会先减少然后增多（也就是退化问题）。
原因如下：当我们初始化神经网络时，所有的层都会随机选择权重，而随着网络逐层推进，激活值
也会不断地乘以随机权重矩阵，当一个网络深度很深时，最终当数据传输到输出层时，我们产生的output实际上会与input产生很大的偏差。而当进行反向传播时，由于output与input的关联性很差，再进行权重偏差调整，也不会产生很好的效果。就算传到了数据偏差小的前几层，由于传回的梯度也已经被许多随机权重的乘法打乱，所以在随后的二次正向传播时，我们对后面层所作的更新也不具有多大意义。综上，当网络很深时，训练时对前对后所作的更新都不是很有意义，因为它们的梯度已经被打乱。所以我们即使通过大量训练，也看不到太大的改进。
为了解决这个问题，根据何恺明等人提出的resnet，我们可以使用skip connection 来实现。
通过skip connection，可以加速训练，并且减少损失。 
![[Pasted image 20241010140940.png]]



示例：
一个两层神经网络，在L层进行激活，得到$a^{[l+1]}$再次进行激活，然后得到$a^{[l+2]}$。

![[Pasted image 20241010143528.png]]

信息流从$a^{[l]}$到$a^{[l+2]}$需要经过以上所有步骤，即这组网络层的主路径。
在ResNet中，我们将让$a^{[l]}$的信息直接到达神经网络的深层，不再沿着主路径（main path）传递。

### resnet的作用？
1.每个残差块的计算变得简单高效
input通过skip connection 可以作用于网络中的中后层，所以现在每一层的工作不再聚焦于需要向后传递input的主要（重要）信息，而是要弄清楚它可以在输入之上添加哪些信息使后续处理更容易。
而这类任务相对来说更容易学习。
2.更短的梯度调节路径（更短的训练时间，更有效的更新）
每个块“block”，都有穿过其中每一层的路径，也有绕过块内中间层的路径。梯度可以随着这两种路径进行流动（调节）。而这时，损失函数也可以通过更短的那条路径回到更靠近始端的层（更有意义的层）去估计损失，从而更有效地更新内容。
3.额外的优势
可以更简单地添加层数，来建构更深入，更强大的神经网络。





