### 什么是主成分？
从众多指标中选取一个作为“主成分”？ NO
主成分是指从众多变量中拟合出尽可能代替更多变量的“变量”，实现从多到少的过程。
所以主成分是原始变量经过降维操作拟合而成的一个“新的变量”，而通过学习PCA的原理，我们可以了解到这一 “新的变量” 是原始多个变量的线性组合。

![Pasted image 20241004083221](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040446.png)
若要表示在图A中的数据点，我们需要X和Y两个坐标轴来表示，而若我们把图A旋转至图B，此时用PC1这一个坐标轴就能表示出数据点之间的差异。


### 如何实现PCA？ 

举个实例，
![Pasted image 20241004110545](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040382.png)

中心化：
![Pasted image 20241004110619](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040545.png)

做出每个投影点到原点平方距离之和最大的一条直线。即为PC1. （直线的斜率可以通过特征向量直接求得）
![Pasted image 20241004110638](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040807.png)
如图所示，这条直线的斜率是0.25，代表样本随着特征x向外移动4个单位，就会随着特征y向外移动1个单位，同时也说明了x比y更影响特征的分布。



![Pasted image 20241004112057](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101041562.png)


把三角形的每一条边都除以4.12，得到下图.红线所在方向向量即为PC1的特征向量或奇异向量。
PC1特征值=$\frac{SS}{N-1}$
PC1奇异值=$\sqrt{SS}$
这里SS指的是 PC1上各个点到原点距离的平方和。

![Pasted image 20241004112335](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101041481.png)

最后，旋转所有点使得PC1水平，即可画出最终的PCA图片。

注：
看上去这个过程和线性回归有几分相似，都是拟合出一条直线。但是pca和线性回归是两回事，它们拟合直线的方式，以及功能/目标都是不同的。如果你的目的是预测y值的大小，那么就选择线性回归，如果你的目的是化简数据降低维度，那么就选择pca。

### 总结

pca的作用总结：

1.数据可视化
pca的一个最大的特点就是可视化，所以我们更经常把n个变量降维到2或3个变量，然后在二维或三维图中可视化这些变量。-->可视化

2.特征提取
两个变量，一个变量在每个样本中的变化程度几乎可以忽略不计，这时候只能利用另一个变化程度大来体现数据来源于不同的样本。  --->特征提取

3.数据压缩
PCA可以将具有较大变化程度的两个变量合并为一个主成分，突出变化显著的部分，同时保留信息较少的变量作为另一个成分。-->数据压缩。


下面来总结一下进行代码实现的具体步骤
拿到数据
第一步，标准化：
	 缩放+中心化。
第三步，计算协方差矩阵的特征值和特征向量：
	特征值解释每个主成分的方差大小，
	特征向量表示每个主成分的方向。
Ps:协方差矩阵
当有2个变量时：
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410130928899.png)
当有3个变量时：
![image.png](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410130928359.png)


第四步，选择主成分：
	 根据特征值的大小，选择前k个最大的特征值（即$\frac{SS}{N-1}$）对应的特征向量作为主成分（使得它们能够保留原始数据中90%到95%的方差）。
第五步，数据投影：
	 设选定的k个主成分的特征向量为$v_1,v_2,\dots,v_k$，则降维后数据 $z$ 可以表示为：$z=XV$
	 $X$是标准化的数据矩阵，$V$是前k个特征向量组成的矩阵。


