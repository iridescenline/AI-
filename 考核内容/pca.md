**低维度的可视化直观理解版本。**
### 引子

![Pasted image 20241004104734](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040878.png)

![Pasted image 20241004104813](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040840.png)

![Pasted image 20241004104826](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040243.png)


![Pasted image 20241004104911](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040222.png)

==随着测量基因数的增加，我们无法直接画出直观的图来表示数据变量
这个时候，就需要用到主成分分析了。


### 什么是主成分？
从众多指标中选取一个作为“主成分”？ NO
主成分是指从众多变量中拟合出尽可能代替更多变量的“变量”，实现从多到少的过程。
所以主成分是原始变量经过降维操作拟合而成的一个“新的变量”，而通过学习PCA的原理，我们可以了解到这一 “新的变量” 是原始多个变量的线性组合。

![Pasted image 20241004083221](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040446.png)
若要表示在图A中的数据点，我们需要X和Y两个坐标轴来表示，而若我们把图A旋转至图B，此时用PC1这一个坐标轴就能表示出数据点之间的差异。




举个实例，
![Pasted image 20241004110545](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040382.png)

中心化：
![Pasted image 20241004110619](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040545.png)

通过随机画出过原点的一条直线，逐步精确到每个投影点到原点平方距离之和最大的一条直线。即为PC1. 
![Pasted image 20241004110638](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040807.png)
如图所示，这条直线的斜率是0.25，代表样本随着特征x向外移动4个单位，就会随着特征y向外移动1个单位，同时也说明了x比y更影响特征的分布。



![Pasted image 20241004112057](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101041562.png)


把三角形的每一条边都除以4.12，得到下图.红线所在方向向量即为PC1的特征向量或奇异向量。
PC1特征值=$\frac{SS}{N-1}$
PC1奇异值=$\sqrt{SS}$
这里SS指的是 PC1上各个点到原点距离的平方和。

![Pasted image 20241004112335](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101041481.png)

最后，旋转所有点使得PC1水平，即可画出最终的PCA图片。

注：
看上去这个过程和线性回归有几分相似，都是拟合出一条直线。但是pca和线性回归是两回事，它们拟合直线的方式，以及功能/目标都是不同的。如果你的目的是预测y值的大小，那么就选择线性回归，如果你的目的是化简数据降低维度，那么就选择pca。

### 总结

下面来总结一下进行代码实现的具体步骤
拿到数据
第一步，标准化：
	 缩放+中心化。
第三步，计算协方差矩阵的特征值和特征向量：
	特征值解释每个主成分的方差大小，
	特征向量表示每个主成分的方向。
第四步，选择主成分：
	 根据特征值的大小，选择前k个最大的特征值（即$\frac{SS}{N-1}$）对应的特征向量作为主成分（使得它们能够保留原始数据中90%到95%的方差）。
第五步，数据投影：
	 设选定的k个主成分的特征向量为$v_1,v_2,\dots,v_k$，则降维后数据 $z$ 可以表示为：$z=XV$
	 $X$是标准化的数据矩阵，$V$是前k个特征向量组成的矩阵。


总结：
两个变量，一个变量在每个样本中的变化程度几乎可以忽略不计，这时候只能利用另一个变化程度大来体现数据来源于不同的样本。  --->忽略那些不变化小的变量，而只关注变化大的变量。
而当两个变量都具备较大的变化程度时，我们还可以主动降维，把这俩变量变换成一个变化量更明显的变量和一个变化程度小的变量的组合，从而达到仅需重点关注一个“变量”的效果。
推广开来，我们可以对n个变量进行主成分分析，达到只关注k（k远小于n）个变量，从而达到简化数据分析的效果。
pca的一个最大的特点就是可视化，所以我们更经常把n个变量降维到2或3个变量，然后在二维或三维图中可视化这些变量。

