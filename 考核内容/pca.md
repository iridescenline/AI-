**低维度的可视化直观理解版本。**
### 引子

![Pasted image 20241004104734](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040878.png)

![Pasted image 20241004104813](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040840.png)

![Pasted image 20241004104826](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040243.png)


![Pasted image 20241004104911](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040222.png)

==随着测量基因数的增加，我们无法直接画出直观的图来表示数据变量
这个时候，就需要用到主成分分析了。


### 什么是主成分？
从众多指标中选取一个作为“主成分”？ NO
主成分是指从众多变量中拟合出尽可能代替更多变量的“变量”，实现从多到少的过程。
所以主成分是原始变量经过降维操作拟合而成的一个“新的变量”，而通过学习PCA的原理，我们可以了解到这一 “新的变量” 是原始多个变量的线性组合。

![Pasted image 20241004083221](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040446.png)
若要表示在图A中的数据点，我们需要X和Y两个坐标轴来表示，而若我们把图A旋转至图B，此时用PC1这一个坐标轴就能表示出数据点之间的差异。

理想情况下（可以降到一维）：
![Pasted image 20241004103202](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410121912851.png)

Ps:上图数据已经过初步处理（中心化）

![Pasted image 20241004103257](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410121913192.png)
![Pasted image 20241004103328](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410121913949.png)

旋转过后的坐标系，横纵坐标不再表示房价和面积，而是两者的“混合”，这时，我们将横纵坐标分别称为PC1和PC2.
![Pasted image 20241004103524](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410121913823.png)


这就是主成分分析，在实际情况中，变量更多，也就更复杂一些，但原理是相同的。

非理想情况下：

![Pasted image 20241004103653](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040047.png)
Ps:上图为经过中心化的数据
![Pasted image 20241004103724](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040083.png)

经过降维，得到下图：

![Pasted image 20241004103922](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040603.png)



### 在上述例子当中，如何找到一直线，最大化地拟合数据？
肉眼观察，当点投影到直线上的投影点到原点距离最大时，拟合效果最好。
并且，在表现形式上，我们采用投影点到原点距离的平方的形式，这样避免负值的出现，否则负值会抵消正值。
例子：
![Pasted image 20241004104813](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040840.png)

![Pasted image 20241004110545](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040382.png)

中心化：
![Pasted image 20241004110619](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040545.png)

通过随机画出过原点的一条直线，逐步精确到投影点平方距离最小的一条直线。即为PC1.
![Pasted image 20241004110638](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101040807.png)
如图所示，这条直线的斜率是0.25，代表样本随着特征x向外移动4个单位，就会随着特征y向外移动1个单位，同时也说明了x比y更影响特征的分布。



![Pasted image 20241004112057](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101041562.png)


把三角形的每一条边都除以4.12，得到下图.红线所在方向向量即为PC1的特征向量或奇异向量。
PC1特征值=$\frac{SS}{N-1}$
PC1奇异值=$\sqrt{SS}$
这里SS指的是 PC1上各个点到原点距离的平方和。

![Pasted image 20241004112335](https://erin-53347-1330131220.cos.ap-guangzhou.myqcloud.com/202410101041481.png)

最后，旋转所有点使得PC1水平，即可画出最终的PCA图片。



以上是直观化理解，下面来总结一下进行代码实现的具体步骤

拿到数据
第一步，标准化：
	 缩放+中心化。
第三步，计算协方差矩阵的特征值和特征向量：
	特征值解释每个主成分的方差大小，
	特征向量表示每个主成分的方向。
第四步，选择主成分：
	 根据特征值的大小，选择前k个最大的特征值对应的特征向量作为主成分（使得它们能够保留原始数据中90%到95%的方差）。
第五步，数据投影：
	 设选定的k个主成分的特征向量为$v_1,v_2,\dots,v_k$，则降维后数据 $z$ 可以表示为：$z=XV$
	 $X$是标准化的数据矩阵，$V$是前k个特征向量组成的矩阵。


PCA 降维的目标维度 k 通常取决于保留的方差比例、应用需求、计算复杂度和可视化需求。
- **示例**：不同应用需求的常见k值如下，
    
    - **图像处理**：50 到 100 维
        
    - **文本分析**：20 到 50 维
        
    - **生物信息学**：10 到 50 维
        
    - **可视化**：2 维或 3 维


笔记来源：b站视频，博客，知乎，deepseek。


